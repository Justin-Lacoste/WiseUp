{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is a helper for testing the extracting process\n",
    "\n",
    "1. Extract PDF\n",
    "\n",
    "2. Extract from YouTube video\n",
    "\n",
    "3. Extract from Audio/Video file\n",
    "\n",
    "4. Extract from zip"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extract PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_file = 'Test\\Harry-Potter-and-the-Methods-of-Rationality.pdf'\n",
    "txt_file = 'Test\\Harry-Potter-and-the-Methods-of-Rationality.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pdfplumber.open(pdf_file)\n",
    "pages = pdf.pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CHAPTER ONE',\n",
       " 'A DAY OF VERY LOW',\n",
       " 'PROBABILIT Y',\n",
       " 'Beneaththemoonlightglintsatinyfragmentofsilver,afractionofaline...',\n",
       " '(blackrobes,falling)',\n",
       " '...bloodspillsoutinliters,andsomeonescreamsaword.',\n",
       " '*',\n",
       " '*',\n",
       " '*',\n",
       " 'Every inch of wall space is covered by a bookcase. Each bookcase',\n",
       " 'has six shelves, going almost to the ceiling. Some bookshelves are',\n",
       " 'stacked to the brim with hardcover books: science, mathematics, his-',\n",
       " 'tory, and everything else. Other shelves have two layers of paperback',\n",
       " 'science ﬁction, with the back layer of books propped up on old tissue',\n",
       " 'boxesortwo-by-fours,sothatyoucanseethebacklayerofbooksabove',\n",
       " 'thebooksinfront. Anditstillisn’tenough. Booksareoverﬂowingonto',\n",
       " 'the tables and the sofas and making little heaps under the windows.',\n",
       " 'Thisistheliving-roomofthehouseoccupiedbytheeminentProfes-',\n",
       " 'sor Michael Verres-Evans, and his wife, Mrs. Petunia Evans-Verres, and',\n",
       " 'their adopted son, Harry James Potter-Evans-Verres.',\n",
       " 'There is a letter lying on the living-room table, and an unstamped',\n",
       " 'envelopeofyellowishparchment,addressedtoMr.H.Potter inemerald-',\n",
       " 'green ink.',\n",
       " 'The Professor and his wife are speaking sharply at each other, but',\n",
       " 'they are not shouting. The Professor considers shouting to be uncivi-',\n",
       " 'lized.',\n",
       " '* 13 *']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[12].extract_text().split(\"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract YouTube transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytube\n",
    "import os\n",
    "import whisper\n",
    "\n",
    "import subprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def words_per_second(model):\n",
    "#     import time\n",
    "#     video_url = 'https://youtu.be/qg4PchTECck'\n",
    "#     start = time.time()\n",
    "#     transcript = get_transcript(model, url=video_url)\n",
    "#     time_taken = time.time() - start\n",
    "#     num_of_words = len(transcript.split())\n",
    "#     return num_of_words / time_taken\n",
    "\n",
    "\n",
    "\n",
    "# models = ['tiny.en', 'base.en', 'small.en', 'medium.en']\n",
    "# for model in models:\n",
    "#     print(f\"Model: {model} | Words per second: {words_per_second(model)}\\n\")\n",
    "\n",
    "# # Vid url to transcript\n",
    "# video_url = 'https://youtu.be/qg4PchTECck'\n",
    "# transcript = get_transcript(url=video_url)\n",
    "\n",
    "# # Vid mp4 to transcript\n",
    "# video_path = '/content/Gradient Descent in 3 minutes.mp4'\n",
    "# transcript = get_transcript(video_path=video_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_length(filename):\n",
    "    result = subprocess.run([\"ffprobe\", \"-v\", \"error\", \"-show_entries\",\n",
    "                             \"format=duration\", \"-of\",\n",
    "                             \"default=noprint_wrappers=1:nokey=1\", filename],\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.STDOUT)\n",
    "    return float(result.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transcript(model, url=None, video_path=None):\n",
    "    if url is None and video_path is None:\n",
    "        raise ValueError(\"Either url or video_path must be specified\")\n",
    "    if video_path is None:\n",
    "        data = pytube.YouTube(url)\n",
    "        video = data.streams.get_highest_resolution()\n",
    "        video_path = video.download()\n",
    "    model = whisper.load_model(model)\n",
    "    result = model.transcribe(video_path, language='english')\n",
    "    os.remove(video_path)\n",
    "    return result[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Video downloaded\n",
      "\n",
      "c:\\Users\\Henri\\Documents\\GitHub\\WiseUp\\src\\Gradient Descent in 3 minutes.mp4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "186.015057 seconds\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Henri\\AppData\\Roaming\\Python\\Python310\\site-packages\\whisper\\transcribe.py:78: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    }
   ],
   "source": [
    "# Vid url to transcript\n",
    "video_url = 'https://youtu.be/qg4PchTECck'\n",
    "transcript = get_transcript(\"tiny.en\", url=video_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Gradient descent is the process by which machines learn how to generate new faces, play hide and seek, and even beat the best humans at games like Dota. But what exactly is Gradient descent? To answer that question, let's say you're trying to train your computer to listen to an audio file and recognize three spoken commands based on a label data set. The first step is to formulate this machine learning task as a mathematical optimization problem. For example, you can work with a neural network whose weights are unknown variables. Whose input is an audio data, and whose output is a vector of size 3 where each entry represents how much the neural network thinks the audio corresponds to each command. Then, for each example in the data set, you can pair the output of the neural network to the ideal output. Take the difference, the square, and then the sum, and you get the cost of a single training example. Click in the sum over all training examples, you get the overall cost function. And the problem now becomes, how do we find the right value of theta that makes this cost function as small as possible? And this is where the second step, or Gradient descent comes in. And you might ask, why do we need to invent an algorithm for this at all? Can't we just plot the function and point to the minimizer? Well, theta usually has many more entries than just two. And in that setting, it's hard to see what the function looks like. So, what can we do? The insight of Gradient descent is that, while we cannot get a look at the whole function all at once, we can easily evaluate the cost function at an arbitrary point. And with a process called backpropagation, we can also evaluate the negative gradient of this cost function. And take a small step in that direction. Gradient descent continues in an iterative fashion. It computes the negative gradient at a new point, takes a step in that direction, and so on and so forth. The intuition behind the rest step is that the gradient of a function gives you the direction that increases that function the most. So by taking a small step in the opposite direction, you can hope to make the function decrease at each iteration. Gradient descent, as I just presented it, is known as Vanille like gradient descent. Because several variants have been developed throughout the years to improve it. In stochastic gradient descent, instead of taking the sum over all training examples when taking the gradient of the cost function, which can be expensive when the number of examples is big, we can take a small random subset at each iteration. Adaptive gradient descent picks a different step size for each component of data. And this can be extremely useful when they take sparse, like text data and image data. Momentum gradient descent keeps track of previously computed gradients to build up momentum and accelerate conversions to the minimizer. This was gradient descent in under 3 minutes. If you liked the video, like and subscribe and see you next time.\n"
     ]
    }
   ],
   "source": [
    "print(transcript)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Extract playlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract Word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
