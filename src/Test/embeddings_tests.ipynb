{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is a helper for testing the embedding process\n",
    "\n",
    "Some code from this notebook was inspired by ```https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import openai\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPLETIONS_MODEL = \"text-davinci-003\"\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Marcelo Chierighini of Brazil won the gold medal in the men's high jump at the 2020 Summer Olympics.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Who won the 2020 Summer Olympics men's high jump?\"\n",
    "\n",
    "openai.Completion.create(\n",
    "    prompt=prompt,\n",
    "    temperature=0,\n",
    "    max_tokens=300,\n",
    "    model=COMPLETIONS_MODEL,\n",
    ")[\"choices\"][0][\"text\"].strip(\" \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sorry, I don't know.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"Answer the question as truthfully as possible, and if you're unsure of the answer, say \"Sorry, I don't know\".\n",
    "\n",
    "Q: Who won the 2020 Summer Olympics men's high jump?\n",
    "A:\"\"\"\n",
    "\n",
    "openai.Completion.create(\n",
    "    prompt=prompt,\n",
    "    temperature=0,\n",
    "    max_tokens=300,\n",
    "    model=COMPLETIONS_MODEL\n",
    ")[\"choices\"][0][\"text\"].strip(\" \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gianmarco Tamberi and Mutaz Essa Barshim emerged as joint winners of the event.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"Answer the question as truthfully as possible using the provided text, and if the answer is not contained within the text below, say \"I don't know\"\n",
    "\n",
    "Context:\n",
    "The men's high jump event at the 2020 Summer Olympics took place between 30 July and 1 August 2021 at the Olympic Stadium.\n",
    "33 athletes from 24 nations competed; the total possible number depended on how many nations would use universality places \n",
    "to enter athletes in addition to the 32 qualifying through mark or ranking (no universality places were used in 2021).\n",
    "Italian athlete Gianmarco Tamberi along with Qatari athlete Mutaz Essa Barshim emerged as joint winners of the event following\n",
    "a tie between both of them as they cleared 2.37m. Both Tamberi and Barshim agreed to share the gold medal in a rare instance\n",
    "where the athletes of different nations had agreed to share the same medal in the history of Olympics. \n",
    "Barshim in particular was heard to ask a competition official \"Can we have two golds?\" in response to being offered a \n",
    "'jump off'. Maksim Nedasekau of Belarus took bronze. The medals were the first ever in the men's high jump for Italy and \n",
    "Belarus, the first gold in the men's high jump for Italy and Qatar, and the third consecutive medal in the men's high jump\n",
    "for Qatar (all by Barshim). Barshim became only the second man to earn three medals in high jump, joining Patrik Sjöberg\n",
    "of Sweden (1984 to 1992).\n",
    "\n",
    "Q: Who won the 2020 Summer Olympics men's high jump?\n",
    "A:\"\"\"\n",
    "\n",
    "openai.Completion.create(\n",
    "    prompt=prompt,\n",
    "    temperature=0,\n",
    "    max_tokens=300,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    model=COMPLETIONS_MODEL\n",
    ")[\"choices\"][0][\"text\"].strip(\" \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3964 rows in the data.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title</th>\n",
       "      <th>heading</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Volleyball at the 2020 Summer Olympics – Men's African qualification</th>\n",
       "      <th>Qualification</th>\n",
       "      <td>Seven CAVB national teams which had not yet qu...</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Athletics at the 2020 Summer Olympics – Women's triple jump</th>\n",
       "      <th>Competition format</th>\n",
       "      <td>The 2020 competition continued to use the two-...</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Swimming at the 2020 Summer Olympics – Men's 200 metre individual medley</th>\n",
       "      <th>Competition format</th>\n",
       "      <td>The competition consists of three rounds: heat...</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Botswana at the 2020 Summer Olympics</th>\n",
       "      <th>Boxing</th>\n",
       "      <td>Botswana entered two boxers into the Olympic t...</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Poland at the 2020 Summer Olympics</th>\n",
       "      <th>Table tennis</th>\n",
       "      <td>Poland entered three athletes into the table t...</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                 content  \\\n",
       "title                                              heading                                                                 \n",
       "Volleyball at the 2020 Summer Olympics – Men's ... Qualification       Seven CAVB national teams which had not yet qu...   \n",
       "Athletics at the 2020 Summer Olympics – Women's... Competition format  The 2020 competition continued to use the two-...   \n",
       "Swimming at the 2020 Summer Olympics – Men's 20... Competition format  The competition consists of three rounds: heat...   \n",
       "Botswana at the 2020 Summer Olympics               Boxing              Botswana entered two boxers into the Olympic t...   \n",
       "Poland at the 2020 Summer Olympics                 Table tennis        Poland entered three athletes into the table t...   \n",
       "\n",
       "                                                                       tokens  \n",
       "title                                              heading                     \n",
       "Volleyball at the 2020 Summer Olympics – Men's ... Qualification           99  \n",
       "Athletics at the 2020 Summer Olympics – Women's... Competition format     102  \n",
       "Swimming at the 2020 Summer Olympics – Men's 20... Competition format      67  \n",
       "Botswana at the 2020 Summer Olympics               Boxing                 102  \n",
       "Poland at the 2020 Summer Olympics                 Table tennis            62  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('https://cdn.openai.com/API/examples/data/olympics_sections_text.csv')\n",
    "df = df.set_index([\"title\", \"heading\"])\n",
    "print(f\"{len(df)} rows in the data.\")\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text: str, model: str=EMBEDDING_MODEL) -> list[float]:\n",
    "    result = openai.Embedding.create(\n",
    "      model=model,\n",
    "      input=text\n",
    "    )\n",
    "    return result[\"data\"][0][\"embedding\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_doc_embeddings(df: pd.DataFrame) -> dict[tuple[str, str], list[float]]:\n",
    "    \"\"\"\n",
    "    Create an embedding for each row in the dataframe using the OpenAI Embeddings API.\n",
    "    \n",
    "    Return a dictionary that maps between each embedding vector and the index of the row that it corresponds to.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        idx: get_embedding(r.content) for idx, r in df.iterrows()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(fname: str) -> dict[tuple[str, str], list[float]]:\n",
    "    \"\"\"\n",
    "    Read the document embeddings and their keys from a CSV.\n",
    "    \n",
    "    fname is the path to a CSV with exactly these named columns: \n",
    "        \"title\", \"heading\", \"0\", \"1\", ... up to the length of the embedding vectors.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(fname, header=0)\n",
    "    max_dim = max([int(c) for c in df.columns if c != \"title\" and c != \"heading\"])\n",
    "    return {\n",
    "           (r.title, r.heading): [r[str(i)] for i in range(max_dim + 1)] for _, r in df.iterrows()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_embeddings = load_embeddings(\"https://cdn.openai.com/API/examples/data/olympics_sections_document_embeddings.csv\")\n",
    "\n",
    "# ===== OR, uncomment the below line to recaculate the embeddings from scratch. ========\n",
    "\n",
    "# document_embeddings = compute_doc_embeddings(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('2020 Summer Olympics', 'Summary') : [0.0037565305829048, -0.0061981128528714, -0.0087078781798481, -0.0071364338509738, -0.0025227521546185]... (1536 entries)\n"
     ]
    }
   ],
   "source": [
    "# An example embedding:\n",
    "example_entry = list(document_embeddings.items())[0]\n",
    "print(f\"{example_entry[0]} : {example_entry[1][:5]}... ({len(example_entry[1])} entries)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_similarity(x: list[float], y: list[float]) -> float:\n",
    "    \"\"\"\n",
    "    Returns the similarity between two vectors.\n",
    "    \n",
    "    Because OpenAI Embeddings are normalized to length 1, the cosine similarity is the same as the dot product.\n",
    "    \"\"\"\n",
    "    return np.dot(np.array(x), np.array(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_document_sections_by_query_similarity(query: str, contexts: dict[(str, str), np.array]) -> list[(float, (str, str))]:\n",
    "    \"\"\"\n",
    "    Find the query embedding for the supplied query, and compare it against all of the pre-calculated document embeddings\n",
    "    to find the most relevant sections. \n",
    "    \n",
    "    Return the list of document sections, sorted by relevance in descending order.\n",
    "    \"\"\"\n",
    "    query_embedding = get_embedding(query)\n",
    "    \n",
    "    document_similarities = sorted([\n",
    "        (vector_similarity(query_embedding, doc_embedding), doc_index) for doc_index, doc_embedding in contexts.items()\n",
    "    ], reverse=True)\n",
    "    \n",
    "    return document_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.8848643084506063,\n",
       "  (\"Athletics at the 2020 Summer Olympics – Men's high jump\", 'Summary')),\n",
       " (0.8633938355935517,\n",
       "  (\"Athletics at the 2020 Summer Olympics – Men's pole vault\", 'Summary')),\n",
       " (0.8616397305838509,\n",
       "  (\"Athletics at the 2020 Summer Olympics – Men's long jump\", 'Summary')),\n",
       " (0.8560523857031264,\n",
       "  (\"Athletics at the 2020 Summer Olympics – Men's triple jump\", 'Summary')),\n",
       " (0.8469039130441239,\n",
       "  (\"Athletics at the 2020 Summer Olympics – Men's 110 metres hurdles\",\n",
       "   'Summary'))]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_document_sections_by_query_similarity(\"Who won the men's high jump?\", document_embeddings)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.8726165220223292,\n",
       "  (\"Athletics at the 2020 Summer Olympics – Women's long jump\", 'Summary')),\n",
       " (0.8682196158313356,\n",
       "  (\"Athletics at the 2020 Summer Olympics – Women's high jump\", 'Summary')),\n",
       " (0.8631915263706721,\n",
       "  (\"Athletics at the 2020 Summer Olympics – Women's pole vault\", 'Summary')),\n",
       " (0.8609374262115408,\n",
       "  (\"Athletics at the 2020 Summer Olympics – Women's triple jump\", 'Summary')),\n",
       " (0.8581515607285686,\n",
       "  (\"Athletics at the 2020 Summer Olympics – Women's 100 metres hurdles\",\n",
       "   'Summary'))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_document_sections_by_query_similarity(\"Who won the women's high jump?\", document_embeddings)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Context separator contains 3 tokens'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SECTION_LEN = 500\n",
    "SEPARATOR = \"\\n* \"\n",
    "ENCODING = \"cl100k_base\"  # encoding for text-embedding-ada-002\n",
    "\n",
    "encoding = tiktoken.get_encoding(ENCODING)\n",
    "separator_len = len(encoding.encode(SEPARATOR))\n",
    "\n",
    "f\"Context separator contains {separator_len} tokens\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_prompt(question: str, context_embeddings: dict, df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Fetch relevant \n",
    "    \"\"\"\n",
    "    most_relevant_document_sections = order_document_sections_by_query_similarity(question, context_embeddings)\n",
    "    \n",
    "    chosen_sections = []\n",
    "    chosen_sections_len = 0\n",
    "    chosen_sections_indexes = []\n",
    "     \n",
    "    for _, section_index in most_relevant_document_sections:\n",
    "        # Add contexts until we run out of space.        \n",
    "        document_section = df.loc[section_index]\n",
    "        \n",
    "        chosen_sections_len += document_section.tokens + separator_len\n",
    "        if chosen_sections_len > MAX_SECTION_LEN:\n",
    "            break\n",
    "            \n",
    "        chosen_sections.append(SEPARATOR + document_section.content.replace(\"\\n\", \" \"))\n",
    "        chosen_sections_indexes.append(str(section_index))\n",
    "            \n",
    "    # Useful diagnostic information\n",
    "    print(f\"Selected {len(chosen_sections)} document sections:\")\n",
    "    print(\"\\n\".join(chosen_sections_indexes))\n",
    "    \n",
    "    header = \"\"\"Answer the question as truthfully as possible using the provided context, and if the answer is not contained within the text below, say \"I don't know.\"\\n\\nContext:\\n\"\"\"\n",
    "    \n",
    "    return header + \"\".join(chosen_sections) + \"\\n\\n Q: \" + question + \"\\n A:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 2 document sections:\n",
      "(\"Athletics at the 2020 Summer Olympics – Men's high jump\", 'Summary')\n",
      "(\"Athletics at the 2020 Summer Olympics – Men's long jump\", 'Summary')\n",
      "===\n",
      " Answer the question as truthfully as possible using the provided context, and if the answer is not contained within the text below, say \"I don't know.\"\n",
      "\n",
      "Context:\n",
      "\n",
      "* The men's high jump event at the 2020 Summer Olympics took place between 30 July and 1 August 2021 at the Olympic Stadium. 33 athletes from 24 nations competed; the total possible number depended on how many nations would use universality places to enter athletes in addition to the 32 qualifying through mark or ranking (no universality places were used in 2021). Italian athlete Gianmarco Tamberi along with Qatari athlete Mutaz Essa Barshim emerged as joint winners of the event following a tie between both of them as they cleared 2.37m. Both Tamberi and Barshim agreed to share the gold medal in a rare instance where the athletes of different nations had agreed to share the same medal in the history of Olympics. Barshim in particular was heard to ask a competition official \"Can we have two golds?\" in response to being offered a 'jump off'. Maksim Nedasekau of Belarus took bronze. The medals were the first ever in the men's high jump for Italy and Belarus, the first gold in the men's high jump for Italy and Qatar, and the third consecutive medal in the men's high jump for Qatar (all by Barshim). Barshim became only the second man to earn three medals in high jump, joining Patrik Sjöberg of Sweden (1984 to 1992).\n",
      "* The men's long jump event at the 2020 Summer Olympics took place between 31 July and 2 August 2021 at the Japan National Stadium. Approximately 35 athletes were expected to compete; the exact number was dependent on how many nations use universality places to enter athletes in addition to the 32 qualifying through time or ranking (1 universality place was used in 2016). 31 athletes from 20 nations competed. Miltiadis Tentoglou won the gold medal, Greece's first medal in the men's long jump. Cuban athletes Juan Miguel Echevarría and Maykel Massó earned silver and bronze, respectively, the nation's first medals in the event since 2008.\n",
      "\n",
      " Q: Who won the 2020 Summer Olympics men's high jump?\n",
      " A:\n"
     ]
    }
   ],
   "source": [
    "prompt = construct_prompt(\n",
    "    \"Who won the 2020 Summer Olympics men's high jump?\",\n",
    "    document_embeddings,\n",
    "    df\n",
    ")\n",
    "\n",
    "print(\"===\\n\", prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPLETIONS_API_PARAMS = {\n",
    "    # We use temperature of 0.0 because it gives the most predictable, factual answer.\n",
    "    \"temperature\": 0.0,\n",
    "    \"max_tokens\": 300,\n",
    "    \"model\": COMPLETIONS_MODEL,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query_with_context(\n",
    "    query: str,\n",
    "    df: pd.DataFrame,\n",
    "    document_embeddings: dict[(str, str), np.array],\n",
    "    show_prompt: bool = False\n",
    ") -> str:\n",
    "    prompt = construct_prompt(\n",
    "        query,\n",
    "        document_embeddings,\n",
    "        df\n",
    "    )\n",
    "    \n",
    "    if show_prompt:\n",
    "        print(prompt)\n",
    "\n",
    "    response = openai.Completion.create(\n",
    "                prompt=prompt,\n",
    "                **COMPLETIONS_API_PARAMS\n",
    "            )\n",
    "\n",
    "    return response[\"choices\"][0][\"text\"].strip(\" \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 2 document sections:\n",
      "(\"Athletics at the 2020 Summer Olympics – Men's high jump\", 'Summary')\n",
      "(\"Athletics at the 2020 Summer Olympics – Men's long jump\", 'Summary')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Gianmarco Tamberi and Mutaz Essa Barshim emerged as joint winners of the event following a tie between both of them as they cleared 2.37m. Both Tamberi and Barshim agreed to share the gold medal.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_query_with_context(\"Who won the 2020 Summer Olympics men's high jump?\", df, document_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 1 document sections:\n",
      "('Concerns and controversies at the 2020 Summer Olympics', 'Summary')\n",
      "\n",
      "Q: Why was the 2020 Summer Olympics originally postponed?\n",
      "A: The 2020 Summer Olympics were originally postponed due to the COVID-19 pandemic.\n"
     ]
    }
   ],
   "source": [
    "query = \"Why was the 2020 Summer Olympics originally postponed?\"\n",
    "answer = answer_query_with_context(query, df, document_embeddings)\n",
    "\n",
    "print(f\"\\nQ: {query}\\nA: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 2 document sections:\n",
      "('2020 Summer Olympics medal table', 'Summary')\n",
      "('List of 2020 Summer Olympics medal winners', 'Summary')\n",
      "\n",
      "Q: In the 2020 Summer Olympics, how many gold medals did the country which won the most medals win?\n",
      "A: The United States won the most medals overall, with 113, and the most gold medals, with 39.\n"
     ]
    }
   ],
   "source": [
    "query = \"In the 2020 Summer Olympics, how many gold medals did the country which won the most medals win?\"\n",
    "answer = answer_query_with_context(query, df, document_embeddings)\n",
    "\n",
    "print(f\"\\nQ: {query}\\nA: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 2 document sections:\n",
      "(\"Athletics at the 2020 Summer Olympics – Men's shot put\", 'Summary')\n",
      "(\"Athletics at the 2020 Summer Olympics – Men's discus throw\", 'Summary')\n",
      "\n",
      "Q: What was unusual about the men’s shotput competition?\n",
      "A: The same three competitors received the same medals in back-to-back editions of the same individual event.\n"
     ]
    }
   ],
   "source": [
    "query = \"What was unusual about the men’s shotput competition?\"\n",
    "answer = answer_query_with_context(query, df, document_embeddings)\n",
    "\n",
    "print(f\"\\nQ: {query}\\nA: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 2 document sections:\n",
      "('Italy at the 2020 Summer Olympics', 'Summary')\n",
      "('San Marino at the 2020 Summer Olympics', 'Summary')\n",
      "\n",
      "Q: In the 2020 Summer Olympics, how many silver medals did Italy win?\n",
      "A: 10 silver medals.\n"
     ]
    }
   ],
   "source": [
    "query = \"In the 2020 Summer Olympics, how many silver medals did Italy win?\"\n",
    "answer = answer_query_with_context(query, df, document_embeddings)\n",
    "\n",
    "print(f\"\\nQ: {query}\\nA: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 3 document sections:\n",
      "('France at the 2020 Summer Olympics', 'Taekwondo')\n",
      "('2020 Summer Olympics medal table', 'Medal count')\n",
      "('Taekwondo at the 2020 Summer Olympics – Qualification', 'Qualification summary')\n",
      "\n",
      "Q: What is the total number of medals won by France, multiplied by the number of Taekwondo medals given out to all countries?\n",
      "Let's think step by step.\n",
      "A: France entered two athletes into the taekwondo competition at the Games. We don't know how many medals France won, so we can't answer this question. I don't know.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the total number of medals won by France, multiplied by the number of Taekwondo medals given out to all countries?\\nLet's think step by step.\"\n",
    "answer = answer_query_with_context(query, df, document_embeddings)\n",
    "\n",
    "print(f\"\\nQ: {query}\\nA: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "from typing import List, Dict\n",
    "import json\n",
    "\n",
    "import pytube\n",
    "import whisper\n",
    "\n",
    "import openai\n",
    "import numpy as np\n",
    "from docx import Document\n",
    "\n",
    "\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Extract:\n",
    "    def __init__(self):\n",
    "        self.text_pages: List[str] = []\n",
    "        \n",
    "    def extract_pages(self, file_or_link_or_str: str, str_type: str) -> List[str]:\n",
    "        if str_type == \"text\":\n",
    "            self.text_pages = self.text2text_pages(file_or_link_or_str)\n",
    "        elif str_type == \"pdf\":\n",
    "            self.text_pages = self.pdf2text(file_or_link_or_str)\n",
    "        elif str_type == \"mp3\":\n",
    "            self.text_pages = self.mp3_to_text(file_or_link_or_str)\n",
    "        elif str_type == \"mp4\":\n",
    "            self.text_pages = self.mp4_to_text(file_or_link_or_str)\n",
    "        elif str_type == \"youtube\":\n",
    "            self.text_pages = self.youtube2text(file_or_link_or_str)\n",
    "        elif str_type == \"github\":\n",
    "            self.text_pages = self.github2text(file_or_link_or_str)\n",
    "        elif str_type == \"docx\":\n",
    "            self.text_pages = self.docx2text(file_or_link_or_str)\n",
    "        \n",
    "        self.reformat_pages()\n",
    "        return self.text_pages\n",
    "            \n",
    "    def text2text_pages(self, text: str, threshold: int=700):\n",
    "        for chunk in text.split('. '):\n",
    "            if self.text_pages and len(chunk)+len(self.text_pages[-1]) < threshold:\n",
    "                self.text_pages[-1] += ' '+chunk+'.'\n",
    "            else:\n",
    "                self.text_pages.append(chunk+'.')\n",
    "        return self.text_pages\n",
    "\n",
    "    def pdf2text(self, pdf_path):\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                text = page.extract_text().replace(\"\\t\", \" \").replace(\"\\n\", \" \").replace(\"\\xa0\", \" \")\n",
    "                self.text_pages.append(text)\n",
    "        return self.text_pages\n",
    "\n",
    "    def mp3_to_text(self, mp3_path):\n",
    "        model = whisper.load_model(WHISPER_MODEL_NAME)\n",
    "        self.text_pages = model.transcribe(mp3_path, language='english')[\"text\"]\n",
    "        return self.text_pages\n",
    "\n",
    "    def mp4_to_text(self, mp4_path):\n",
    "        model = whisper.load_model(WHISPER_MODEL_NAME)\n",
    "        self.text_pages = model.transcribe(mp4_path, language='english')[\"text\"]\n",
    "        return self.text_pages\n",
    "\n",
    "    def youtube2text(self, youtube_link):\n",
    "        data = pytube.YouTube(youtube_link)\n",
    "        video = data.streams.get_highest_resolution()\n",
    "        video_path = video.download()\n",
    "        return self.mp4_to_text(video_path)\n",
    "\n",
    "    def github2text(self, github_repo_link):\n",
    "        raise NotImplementedError(\"github2text\")\n",
    "\n",
    "    def word_office_to_text(self, word_file_path):\n",
    "        document = Document(word_file_path)\n",
    "        for para in document.paragraphs:\n",
    "            self.text_pages.append(para.text)\n",
    "        return self.text_pages\n",
    "    \n",
    "    def reformat_pages(self):\n",
    "        low_thresh = 150\n",
    "        high_thresh = 750\n",
    "        \n",
    "        reformatted_pages = [\"\"]\n",
    "        for page in self.text_pages:\n",
    "            words_in_last_page = len(reformatted_pages[-1].split())\n",
    "            words_in_cur_page = len(page.split())\n",
    "\n",
    "            #condition 1, add page to last page\n",
    "            if (words_in_last_page < low_thresh) and (words_in_last_page + 1 + words_in_cur_page < high_thresh):\n",
    "                reformatted_pages[-1] += f\"\\n{page}\"\n",
    "            \n",
    "            #condition 2, page too big, split in two\n",
    "            elif (words_in_cur_page > high_thresh):\n",
    "                half_page_i = len(page)//2\n",
    "                reformatted_pages.append(page[:half_page_i])\n",
    "                reformatted_pages.append(page[half_page_i:])\n",
    "            \n",
    "            #condition 3, add the page to a new reformatting page\n",
    "            else:\n",
    "                reformatted_pages.append(page)\n",
    "        self.text_pages = reformatted_pages\n",
    "            \n",
    "    def get_dict(self):\n",
    "        # This program takes a list of strings, and returns a dictionary in the following format: {\"pages_text\": [\"page1\", \"page2\", ...], \"pages_embeddings\": arr.tolist()}\n",
    "        \n",
    "        def get_embedding(page: str):\n",
    "            result = openai.Embedding.create(model=config.EMBEDDING_MODEL, input=page)\n",
    "            return result[\"data\"][0][\"embedding\"]\n",
    "\n",
    "        arr = np.array([get_embedding(page) for page in self.text_pages])\n",
    "        return self.text_pages, arr.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "extract = Extract()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_file = \"Test\\AIPosNegFactor.pdf\"\n",
    "pdf_file = r\"C:\\Users\\Henri\\Documents\\GitHub\\WiseUp\\src\\Test\\phil.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pages = extract.extract_pages(pdf_file, \"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.EMBEDDING_MODEL = \"text-embedding-ada-002\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = \"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\"\n",
    "\n",
    "text_pages, embedding_pages = extract.get_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_similarity(x: list[float], y: list[float]) -> float:\n",
    "    \"\"\"\n",
    "    Returns the similarity between two vectors.\n",
    "    \n",
    "    Because OpenAI Embeddings are normalized to length 1, the cosine similarity is the same as the dot product.\n",
    "    \"\"\"\n",
    "    return np.dot(np.array(x), np.array(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9565584663693785"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_similarity(embedding_pages[0], embedding_pages[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text: str) -> list[float]:\n",
    "    result = openai.Embedding.create(\n",
    "      model=config.EMBEDDING_MODEL,\n",
    "      input=text\n",
    "    )\n",
    "    return result[\"data\"][0][\"embedding\"]\n",
    "\n",
    "def order_document_sections_by_query_similarity(query: str, contexts):\n",
    "    query_embedding = get_embedding(query)\n",
    "    \n",
    "    document_similarities = sorted([(vector_similarity(query_embedding, get_embedding(context)), context) for context in contexts\n",
    "    ], reverse=True)\n",
    "    \n",
    "    return document_similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = order_document_sections_by_query_similarity(\"Bayesian probabilities\", text_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8401311861016638\n",
      "this with a simple example. Suppose you are wondering how likely it is to rain during an  upcoming tennis match. The problem is that you don’t remember where the tennis match  will take place. You think it might be in New York or Boston or LA. Your credences are  as follows:      Cr (NY) = 0.48    Cr (Boston) = 0.48    Cr (LA) = 0.04    Of course, how likely it is to rain during the match depends on where it will take place.  You have the following conditional credences reflecting this:      Cr (rain|NY) = 0.7    Cr (rain|Boston) = 0.9    Cr (rain|LA) = 0.1    In order to correctly compute Cr(rain), you need to plug your conditional credences of  the form (rain|place of match) and your unconditional credences about where the match  happens into the total probability theorem:    Cr (rain) = Cr(rain|NY) Cr(NY) + Cr(rain|Boston) Cr(Boston) + Cr(rain|LA) Cr(LA)  Cr (rain) = 0.772    This computation could be simplified if you disregarded the possibility that the match  might be in LA, which you consider to be very improbable. If you reasoned with an  outright belief that the match is in Boston or New York, which you consider to be equally  likely, you could simply take the average between the rain probabilities in Boston and  New York, which comes out to Cr(rain) = 0.8. Of course this is not quite correct, but it’s  very close to the correct answer. Formally, we might represent your reasoning as follows  then, representing outright belief as credence 1 in context:                                                                                                                                                                        simplify reasoning processes. In the philosophical literature, acceptances and outright beliefs are commonly  distinguished because they differ in how they are formed. Acceptance is usually taken to be under a  thinker’s voluntary control. I can decide to treat any claim as true in reasoning, no matter how low my  confidence in the claim is, while being fully aware that the claim might be false. By contrast, outright beliefs  cannot be voluntarily adopted in the same way (although some philosophers think we have some amount of  control over what we believe). Rather, whether or not we outright believe a claim is usually regulated by  cognitive processes that are automatic. We seem to have some deliberative control over switching from  relying on an outright belief to relying on a credence, for example by directing our attention to ways in  which we might be mistaken. But we usually can’t employ deliberative control over which claims we take  for  granted in  framing  a  reasoning  problem,  this  is  done  automatically  and  without  our  conscious  awareness. The Bayesian challenge, as I understand it, is the question of why our minds are equipped to  employ outright beliefs, so construed, and this paper discusses one possible answer and its implications.      5 \n",
      "0.8391298627263706\n",
      "conditional credences should be stable (Zhao & Osherson 2010). Moreover, conditional  probability judgments differ (contrary to what would be rational according to Bayesian  models) depending on whether subjects learn that some claim p is true, or are asked to  suppose that p is true (Zhao et al. 2012). People’s conditional probability judgments also  don’t align perfectly with their unconditional probability judgment as required by the  standard ratio formula (Zhao et al. 2009; see also Evans et al. 2015 for further helpful  discussion  and  references).  Taken  together,  the  findings  in  the  psychology  literature  suggest  that  the  way  in  which  people  update  on  newly  learned  evidence  often  approximates conditionalization, but mostly doesn’t match it perfectly. It is an ongoing  research  program  to  identify  the  reasoning  strategies  that  give  rise  to  the  observed  updating patterns, but most researchers share the working hypothesis that the underlying  reasoning strategies are not equivalent to the conditionalization rule; rather, it seems  plausible  that  reasoners  use  simplified  strategies  that  approximate  conditionalization  closely enough.    The  hypothesis  that  reasoners  use  simpler  updating  strategies  than  standard  conditionalization gains support from results in computer science and complexity theory.  If the conditionalization rule were relatively simple to implement from a computational  point of view, then it would be puzzling why human reasoners don’t conform to it. Yet, it  turns out that conditionalization can be very complex to execute, which explains why  limited reasoners may rely on simpler rules instead. The most general and abstract result  that  is  relevant  for  our  purposes  is  that  probabilistic  inference,  including  conditionalization, is NP-hard (Cooper 1990). This means that in general, the reasoning  task of determining the probability of some claim p can be solved by a non-deterministic  Turing machine in polynominal time. In other words, the maximum amount of time it  takes  a  non-deterministic  Turing  machine  to solve  the  problem  is  determined  by  a  polynominal  function  of the  size  of the input. A  thinker’s  credences  (or  a  Bayesian  network, which is Cooper’s framework for representing probabilistic information) may  contain all the necessary information to determine Cr (p), but many computational steps  might be required to actually arrive at the result.9 This is of course not to say that every  single probabilistic reasoning task is this complex. This result about NP-hardness is a  result concerning the complexity of probabilistic reasoning (including conditionalization)  in general, not a result about particular instances of reasoning. Whether a particular  reasoning problem is difficult depends on the information that is explicitly available to the  thinker, and the kinds of computations required for arriving at the relevant answer.  Consider the tennis example again, in which the thinker wants to answer the question of                                                           9 According to Cooper, the hardest probabilistic inference problems are ones that are representable by  multiply connected belief networks that contain many uninstantiated variables. This means that the  probability of any given claim in the network can depend on the probabilities of more than one other claim,  and that many claims in the network have non-extreme probability values. Probabilistic reasoning task are  easier when they are representable by singly connected networks, or when the truth-values of many of the  claims in the network are known. The latter finding is in agreement with the thesis that outright beliefs  simplify reasoning, since they let the reasoner assume that their truth values are known.    10 \n",
      "0.8353183388044315\n",
      "URL = <https://plato.stanford.edu/archives/spr2016/entries/frame-problem/>    Slovic, Paul & Liechtenstein, Sarah (1971): “Comparison of Bayesian and Regression  Approaches to the Study of Information Processing in Judgment,” Organizational Behavior  and Human Performance 6, 649–744.    Staffel, Julia (2017): “Accuracy for Believers,” Episteme 14 (1), 39–48.    Staffel, Julia (2016): “Beliefs, buses and lotteries: Why rational belief can’t be stably high  credence,” Philosophical Studies 173 (7), 1721–1734.    Sturgeon, Scott (2015): “The Tale of Bella and Creda,” Philosophers’ Imprint 15 (31), 1-9.      Tang, Weng Hong (2015): “Belief and Cognitive Limitations,” Philosophical Studies 172 (1),  249–260.    Tentori, Katya, Chater, Nick & Crupi, Vincenzo (2016): “Judging the Probability of  Hypotheses Versus the Impact of Evidence: Which Form of Inductive Inference is More  Accurate and Time-Consistent?” Cognitive Science 40, 758–778.    Weatherson, Brian (2016): “Games, Beliefs and Credences,” Philosophy and Phenomenological  Research 92 (2), 209–236.    Wedgwood, Ralph (2012): “Outright Belief,” Dialectica 66 (3), 309–329.    Weisberg, Jonathan (forthcoming): “Belief in Psyontology”, Philosophers’ Imprint.    Weisberg, Jonathan (2013): “Knowledge in Action,” Philosophers’ Imprint 13 (22), 1-23.    Williamson, Timothy (2000): Knowledge and Its Limits. Oxford: Oxford University Press.    Zhao, Jiaying, Crupi, Vincenzo, Tentori, Katya, Fitelson, Branden & Osherson, Daniel  (2012): “Updating: Learning versus supposing,” Cognition 124, 373–378.    Zhao,  Jiaying  &  Osherson,  Daniel  (2010):  “Updating  beliefs  in  light  of  uncertain  evidence: Descriptive assessment of Jeffrey’s rule,” Thinking & Reasoning 16 (4), 288–307.    Zhao, Jiaying, Shah, Anuj & Osherson, Daniel (2009): “On the provenance of judgments  of conditional probability,” Cognition 113, 26–36.        29 \n",
      "0.8194245823235375\n",
      "    Cr (NY) = 0.5    Cr (Boston) = 0.5    Cr (LA) = 0.04      Cr (rain) = Cr(rain|NY) Cr(NY) + Cr(rain|Boston) Cr(Boston)     Cr (rain) = (0.7 + 0.9) 0.5 = 0.8    This example is a very simple illustration of the general idea that reducing the number of  possibilities under consideration simplifies reasoning problems, both of the theoretical and  the practical kind. This observation helps explain why it makes sense for limited human  reasoners to have outright beliefs in addition to credences: outright beliefs let us eliminate  improbable options from consideration in framing reasoning problems, thus making them  easier to solve. We will capture this response to the Bayesian challenge in claim (1):    (1) Human thinkers have outright beliefs in addition to credences because outright beliefs  help simplify reasoning processes.      In order to show that (1) leads to a puzzle, I need to introduce three further claims. I will  state them first, then explain why each of them is plausible, and then argue that they give  rise to a puzzle when combined with (1).     (2) Reasoning processes can involve mixtures of credences and outright beliefs, and it is  flexible from context to context which outright beliefs are relied upon.    (3) Outright beliefs and credences in a context are determined by pseudo-conditionalizing  on a set of background credences.    (4)  Pseudo-conditionalizing  is  difficult  to  execute  for  human  reasoners,  because  it  is  computationally expensive.    Claim (2) consists of two sub-claims, which we might call Mixing and Switching. According  to Mixing, any given reasoning task can involve both outright beliefs and credences. The  tennis  match  example  illustrates  this  claim:  you  rely  on  an  outright  belief  in  the  disjunction that the match is in New  York or Boston, but you also rely on various  conditional and unconditional credences when determining how confident you should be  that it will rain during the match. Many, perhaps even most, practical and theoretical  reasoning problems have this structure, in which some contingent claims are treated as  true, and some claims are treated as uncertain in generating an answer to the problem.  According to Switching, whether or not we rely on a credence or on an outright  belief in a relevant claim can change from context to context. (In a given context, we    6 \n",
      "0.8139523059856038\n",
      "how likely it is to rain during the match. If the thinker already knew, for example, how  likely it was not to rain during the match, the inference to the probability of rain would be  trivial.  Similarly  for  pseudo-conditionalization:  If  the  thinker  already  had  all  of  the  conditional  probabilities  needed  for  pseudo-conditionalizing  in  any  context  readily  available, then pseudo-conditionalizing would not be very difficult, because PC would  basically require a trivial one-step inference.10 But this assumption is not psychologically  realistic. Humans have some conditional and unconditional credences readily available to  them, whereas others can only be generated by inferential processes (Kirsh 2003). If  thinkers  had  the  conditional  credences  needed  for  pseudo-conditionalizing  readily  available to them, then they should also have them readily available for use in regular  conditionalization. Yet, as we know from the psychological research I briefly summarized  above,  human  thinkers  at  best  approximate  correct  Bayesian  inferences,  including  conditionalization  in  their  reasoning.  It  is  a  matter  of  active  debate  how  we  can  characterize the algorithm human thinkers use when reasoning with and updating their  credences, but there seems to be little doubt that it is some kind of heuristic and not a full  Bayesian algorithm. Hence, if being a perfect conditionalizer is infeasible for a human  thinker, being a perfect pseudo-conditionalizer is, too.   We now have the resources to see why claims (1)-(4) together generate a puzzle.  While they are not inconsistent, it is hard to see how they could be true at the same time.  Claim (1) asserts that the reason why human thinkers are equipped with outright beliefs in  addition to credences is that outright beliefs simplify reasoning tasks, and thereby make  them more tractable for limited beings like us. But if human thinkers have to use pseudo- conditionalization for managing their credences and outright beliefs across contexts, then  the  computational  cost  of  using  this  strategy  is  likely  to  significantly  diminish  the  computational benefits from having outright beliefs in the first place. Hence, it is very  implausible that the claims in the puzzle are jointly true. In the next section, I will  examine which of the four claims should be rejected.       2. Solving the Puzzle  The first  claim  in  the  puzzle  states that  people  have  outright  beliefs in  addition  to  credences in their inventory of doxastic attitudes, because they help rein in the complexity  of our reasoning. There are different strategies for denying this claim. The first option is  of course to deny its presupposition, i.e. that people have outright beliefs at all. On such a                                                           10 A similar picture is suggested for consideration by an anonymous referee. If humans had all of their  conditional credences explicitly stored, as well as all the claims to which they assign credence 1 or 0, then  any unconditional credence could be arrived at via a trivial one-step inference. On this view, computational  effort is reduced by drastically increasing the amount of information that needs to be stored by the thinker.  The thinker needs to have a conditional credence “ready to go” for every possible evidential situation she  might find herself in. Human thinkers don’t have this kind of extensive store of conditional credences, so  this way of making PC feasible doesn’t work for us. This problem was already recognized by Harman, who  argues that humans can’t update their credences via conditionalization (1986, p. 25). However, Harman  mistakenly assumes that this is the only way in which reasoning with degrees of belief could be implemented  by humans, and on this basis, rejects the possibility that humans can reason with credences altogether.     11 \n",
      "0.8109202179403205\n",
      "Mastropasqua, Tommaso, Crupi, Vincenzo & Tentori, Katya (2010): “Broadening the  study of inductive reasoning: Confirmation judgments with uncertain evidence,” Memory  & Cognition 38 (7), 941–950.    Moss, Sarah (2018): Probabilistic Knowledge, Oxford: Oxford University Press.    Mugg,  Joshua  (2016):  “The  dual-process  turn:  How  recent  defenses  of  dual-process  theories of reasoning fail,” Philosophical Psychology 29 (2), 300–309.    Nagel,  Jennifer  (2011):  “The  Psychological  Basis  of  the  Harman-Vogel  Paradox,”  Philosophers’ Imprint 11 (5), 1–28.    Norby, Aaron (2015): “Uncertainty Without All the Doubt,” Mind and Language 30 (1), 70– 94.    Pettigrew, Richard (2016): Accuracy and the Laws of Credence. Oxford: Oxford University  Press.    Pettigrew, Richard (2017): “Précis and Replies to Contributors for Book Symposium on  Accuracy and the Laws of Credence,” Episteme 14 (1), 1–30.    Predd, J.B., Osherson, D., Kulkarni, S. & Poor, H. V. (2008): “Aggregating Probabilistic  Forecasts from Incoherent and Abstaining Experts,” Decision Analysis 5 (4), 177–189.    Roeber, Blake (2016): “The Pragmatic Encroachment Debate,” Noûs. (online first)    Ross,  Jacob  &  Schroeder,  Mark  (2014):  “Belief,  Credence,  and  Pragmatic  Encroachment,” Philosophy and Phenomenological Research, Vol. 88, No. 2, 259–288.    Schervish, Mark, Seidenfeld, Teddy & Kadane, Joseph (2000): “How Sets of Coherent  Probabilities May Serve as Models for Degrees of Incoherence,” International Journal of  Uncertainty, Fuzziness and Knowlegde-based Systems 8, 347–355.    Schervish, Mark, Seidenfeld, Teddy & Kadane, Joseph (2002): “Measuring Incoherence.”  Sankhya: The Indian Journal of Statistics 64, 561–587.    Schervish,  Mark,  Seidenfeld,  Teddy  &  Kadane,  Joseph  (2003):  “Measures  of  Incoherence: How not to Gamble if You Must,” in: Bayesian Statistics 7, ed. José Bernardo,  et. al., 385–401. Oxford: Oxford University Press.  Shanahan, Murray (2016): “The Frame Problem,” The Stanford Encyclopedia of Philosophy,  Spring 2016 Edition, ed. Edward N. Zalta.     28 \n",
      "0.8051316867003765\n",
      "introduce only a fairly minimal amount of incoherence, the tradeoff can be beneficial for  the thinker. I further argued that we can draw an important normative lesson from this  solution to the puzzle. Endorsing the heuristic view of outright belief captured in claim (1)  of the puzzle entails that putative norms on belief can’t be defended on purely epistemic  grounds.  They  must  also  be  vetted  for  their  complexity  and  feasibility  for  human  reasoners. If outright beliefs’ primary function is to simplify reasoning, they can’t possibly  be governed by norms that prevent them from accomplishing this.     Acknowledgements:  I  would  like to thank  Brian  Talbot,  Daniel  Greco,  Hanti  Lin,  Elizabeth  Schechter,  Kathryn Lindeman, Ralph Wedgwood, Kenny Easwaran, Richard Pettigrew, Catrin  Campbell-Moore, Jason Konek, Ben Levinstein, Alan Hájek, Branden Fitelson, Teddy  Seidenfeld,  and  an  anonymous  referee,  as  well  as  audiences  at  CU  Boulder,  the  University of Edinburgh, the London School of Economics, ECAP 9, Carnegie Mellon  University,  and  the  University  of  Southern  California  for  helpful  comments  and  discussion.    References  Brown, Jessica (2008): “Subject-Sensitive Invariantism and the Knowledge Norm for  Practical Reasoning,” Noûs 42 (2), 167–189.    Buchak, Lara (2014): “Belief, Credence, and Norms,” Philosophical Studies 169 (2), 285– 311.    Carr,  Jennifer  (forthcoming):  “Subjective  Probability  and  the  Content/Attitude  Distinction,” Oxford Studies in Epistemology.    Clarke, Roger (2013): “Belief Is Credence One (In Context)”, Philosophers’ Imprint 13, 1– 18.     Cooper, Gregory F. (1990): “The Computational Complexity of Probabilistic Inference  Using Bayesian Belief Networks”, Artificial Intelligence 42 (2-3), 393-405.    Dallmann, Justin (2017): “When Obstinacy is a Better (Cognitive) Policy”, Philosophers’  Imprint 17 (24), 1–17.    De Bona, Glauber & Finger, Marcelo (2015) “Measuring Inconsistency in Probabilistic  Logic: Rationality Postulates and Dutch Book Interpretation,” Artificial Intelligence 227,  140–64.      25 \n",
      "0.7998457326209534\n",
      "Cr(~A&~B) = 0.03    Cr(A) = 0.96  Cr(~A) = 0.04  Cr(B) = 0.87  Cr(~B) = 0.13    Suppose that the thinker eliminates some of the possibilities by using either GR or SR. If  the remaining, unrounded credences were determined in the same way, regardless of  whether some possibilities are eliminated from consideration, the thinker is left with an  incoherent  credence  function  within  a  context.  Although  this  kind  of  procedure  introduces slight incoherence, it eliminates the computational cost of renormalizing the  remaining credences depending on which possibilities are considered live, and is thus less  difficult to execute than PC.  Below, the table shows how the different rules would determine the thinker’s  credences in a given context. The columns PC(A) and PC(B) show the thinker’s attitudes  if she pseudo-conditionalized on A and B, respectively. The columns GR(>0.85) and  GR(>0.9) show how the thinker’s attitudes would turn out as a result of applying the  General Rounding rule with different thresholds. The columns SR(A) and SR(B) show the  thinker’s attitudes resulting from selective rounding, treating either only A or only B as  true.    Credences only  PC(A)  PC(B)  GR  GR  SR(A)  SR(B)  (>0.85)   (>0.9)  Cr(A&B) = 0.86  0.895  0.989  1  0.86  0.86  0.86  Cr(A&~B) = 0.1  0.105  0  0  0.1  0.1  0  Cr(~A&B) = 0.01  0  0.011  0  0  0  0.001  Cr(~A&~B) = 0.03  0  0  0  0  0  0  Cr(A) = 0.96  1  0.989  1  1  1  0.96  Cr(~A) = 0.04  0  0.011  0  0  0  0.04  Cr(B) = 0.87  0.895  1  1  0.87  0.87  1  Cr(~B) = 0.13  0.105  0  0  0.13  0.13  0    Looking at these results, we can easily see that since they don’t renormalize the remaining  non-extreme credences, they tend to generate some incoherence in the thinker’s credence  function within a context of reasoning.     There is much more to be learned about how humans store beliefs and credences,  and how they reason with them. Examining some sample theories has been instructive in  shedding light on open questions in this area. Studying the Norby/Weisberg proposal is  instructive, because we saw how their sketch of the mental architecture of human belief    21 \n",
      "0.7994467099607394\n",
      "thinker’s memory and the recall episode. If this happens for each possibility separately, we  could end up with slightly incoherent credences even within a context of reasoning.  Here’s how this might happen. Suppose a thinker’s full possibility space regarding some  issue E (say, where the tennis match will take place) contains five possibilities, E  – E . If  1 5 all possibilities were considered, E  and E  would be considered very unlikely, say 5%  1 2 likely each. E  would be considered the most likely, say 50%, and the remaining two  3 possibilities are each considered 20% likely. Now, in some contexts, the thinker might not  even consider E  and E at all. But if the mechanism for assigning credences to the  1 2  remaining three options remains the same as in a context in which all five possibilities are  considered, then the thinker might end up assigning credences to E , E  and E  that sum  3 4 5 to slightly less than 100%. Unless some procedure is in place that ensures that the  credences in the options under consideration are normalized, so that they sum to 100%,  the “recall and assign each credence on the fly” procedure does not guarantee coherence  within a context. Such a normalization procedure requires computational effort, so it is a  place in which our mind might cut corners. Of course, it is important to emphasize again  that whether we employ such a normalization procedure or not is ultimately an empirical  question. But as described, the Norby/Weisberg picture does not rule out the possibility  of incoherence within a context. If there is no normalization procedure that coherentizes  the credences in a given context, we again arrive at deviations from PC, this time within  contexts of reasoning.     In  the  literature  on  the  relationship  between  credences  and  beliefs,  some  philosophers have proposed threshold rules that determine what is believed in a context.  For example, Foley (2009) proposes a descriptive version of the Lockean thesis, which  says that each context is associated with a credence threshold, such that “one believes that  P just in case one is sufficiently confident of the truth of P.” Call this rule General Rounding  (GR). Alternatively, there could be a rule according to which a thinker treats only some  claims as true (false) that are assigned a credence above (below) some threshold. Call this  rule Selective Rounding (SR).14 Questions concerning the complexity of the implementation  of such rules, and how this affects the coherence of a thinker’s attitude, don’t usually get  explicitly discussed. But it is worth noting that if those rules were implemented without  employing an additional renormalization procedure, they would also lead to incoherence  within contexts. We can see the effects that failures to renormalize have on the coherence  of  a  thinker’s  credences  by  considering  an  example.  Here  are  some  credences  that  constitute  a  subset  of  some  thinker’s  full  credence  function.  Within  this  subset,  no  possibilities have currently been ruled out for the purpose of simplifying reasoning:    Cr(A&B) = 0.86  Cr(A&~B) = 0.1  Cr(~A&B) = 0.01                                                           14 I am grateful to Sylvia Wenmackers for helping me see that GR and SR should be distinguished.    20 \n",
      "0.7959906994401496\n",
      "view, which has been defended by Jeffrey (1970) and Pettigrew (2016) among others,  there is no puzzle to begin with, because the question of what beliefs are for doesn’t arise.  This solution is of course unattractive for the many defenders of the view that we have  outright beliefs. This paper is not the place to settle the debate about whether people  really have outright beliefs. I will proceed on the assumption that human thinkers do in  fact have outright beliefs that can play a role in their reasoning and decision-making, and  note that this view is not universally endorsed. One might also consider challenging the  idea that cutting down the space of possibilities simplifies reasoning tasks. Yet, since this  result has a solid footing in complexity theory (Cooper 1990), this response is implausible.  This leaves us with the option of denying claim (1) by arguing that human thinkers have  outright beliefs for a different reason. On such a view, their main function is something  other than simplifying reasoning.   Some alternative explanations of the function of outright beliefs have recently  been proposed in the literature. One view is that outright beliefs are necessary as a basis  for moral judgment. Buchak (2014) argues that merely being very confident that, say,  Hans  didn’t  pay  for  his  concert  ticket,  is  not  sufficient  for  judging  that  Hans  did  something wrong. This is because high confidence can sometimes be justified based on  purely statistical evidence (for example evidence that most concert attendees used forged  tickets), but purely statistical evidence is an intuitively (and legally) insufficient basis for a  judgment of wrongdoing. Buchak argues that outright beliefs are sensitive to different  evidence types, and that one can only rationally form an outright belief in a claim if one  possesses non-statistical evidence in its support. Hence, a rational outright belief in a  claim p can form the basis of a moral judgment, since it ensures that the thinker possesses  non-statistical evidence in support of p. By contrast, a high credence in p may be based  on purely statistical evidence, and thus cannot always be an appropriate basis for a moral  judgment. While I am sympathetic to Buchak’s argument that purely statistical evidence  is insufficient as a basis for moral judgment, her position is less convincing as a response  to the Bayesian challenge. The worry, in brief, is this: Suppose high confidence based on  non-statistical  evidence  can  rationalize  outright  belief,  and  outright  belief  is  the  appropriate basis of moral judgment. Why, on this view, is outright belief needed as a  middleman? Why can’t high confidence based on non-statistical evidence directly support  moral judgments? This problem with Buchak’s view is especially salient when we consider  her analogy with legal cases. As Moss (2018) points out, in civil cases, the “preponderance  of evidence” standard applies, which means that in order to win a case, the relevant party  in the trial must show that the evidence makes it more than 50% likely that they are right.  The evidence in question that gives rise to this probability must not be purely statistical.  In a case that is decided according to this standard of proof, the resulting probability  might not be high enough to warrant belief that the winning party is right. Hence, what  matters for whether a legal judgment can be reached in this kind of case is whether the  relevant claim can be established to be more than 50% likely based on the right kind of  evidence, not whether one can form an outright justified belief in the relevant claim.    12 \n"
     ]
    }
   ],
   "source": [
    "for i in a[:10]:\n",
    "    print(i[0])\n",
    "    print(i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
